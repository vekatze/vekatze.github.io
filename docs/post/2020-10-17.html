<!DOCTYPE html>
<html>
    <head>
        <title>Executing Types - 以析比域</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <meta property="og:url" content="https://vekatze.github.io/post/2020-10-17.html" />
        <meta property="og:title" content="Executing Types - 以析比域" />
        <meta property="og:type" content="article" />
        <meta property="og:image" content="https://vekatze.github.io/media/vekatze.jpg" />
        <link rel="stylesheet" href="/style.css" />
        
        
        <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
    </head>
    <body>
        <header>
            <h1><a href="/index.html">以析比域</a></h1>
        </header>
        <main>
            <header>
    <h1>Executing Types</h1>
    
    <span class="metadata">2020-10-17</span>
    
</header>

<p>I was looking for a language that satisfies all of the following:</p>
<ol>
<li>Its type system is static and strong enough<br />
</li>
<li>It corresponds to the ordinary lambda calculus<br />
</li>
<li>Its memory management is determined at compile-time</li>
</ol>
<p>Such a thing can't be found easily, of course. The third condition is notably problematic. If we consider only the first two, we immediately reach our happy ending by simply holding hands with garbage collection. I'll be in a peaceful life with OCaml, Haskell, or F# in this case. When we consider the third one, however, the plot gets twisted.</p>
<p>In the end, <a href="https://github.com/u2zv1wx/neut">I implemented such a language by myself</a>. A dependently-typed programming language with static memory managemement that doesn't rely on additional annotations to its type system. It seemed to me that the method that I found to pass the third condition was interesting by itself, and that's why I decided to introduce it in this article.</p>
<p>Getting straight to the point, the method is something that "executes types." Or, more specifically, the method converts a type into a function that copies / discards the values of the type. I emphasize here that I do love all of the GC-based approaches, the region-based approaches, and the manual approach that we see in C. All of them in their own ways. Be that as it may, I think the approach that I present in this article is interesting as a possibility.</p>
<p>Some notes:</p>
<ul>
<li>This article doesn't contain any safety guarantees. Obviously I agree that those guarantees would make this article 100 times better. This omission is simply due to my limited resource. I suggest you take this article as an implication or something like that, with a grain of salt.<br />
</li>
<li>I chose to write the motivation and the background in the appendix; The main part focuses on describing the point of the method as quickly as possible. You can visit the appendix to fully enjoy the virtue of the method.<br />
</li>
<li>This article should be read by anyone who knows OCaml, Haskell, or a language that is similar to them. If it isn't, it's simply my fault.</li>
</ul>
<p>Let's move on to the main part. It starts by constructing a small language with desirable properties.</p>
<h2 data-toc=":include siblings :depth 2 :ignore (this)" id="toc">Index</h2>
<div class="CONTENTS drawer">
<ol>
<li><a href="#exploiting-the-power-of-linearity">Exploiting the power of linearity</a>
<ol>
<li><a href="#a-language-that-uses-every-variable-just-once">A language that uses every variable just once</a><br />
</li>
<li><a href="#resource-management-under-linearity">Resource management under linearity</a><br />
</li>
<li><a href="#smuggling-non-linearity">Smuggling non-linearity</a><br />
</li>
</ol></li>
<li><a href="#executing-types">Executing Types</a>
<ol>
<li><a href="#finding-vocabulary-for-resource-management-in-the-target-language">Finding vocabulary for resource management in the target language</a><br />
</li>
<li><a href="#how-types-are-translated-more-concretely">How types are translated, more concretely</a><br />
</li>
<li><a href="#how-translated-types-are-utilized">How translated types are utilized</a><br />
</li>
<li><a href="#a-short-break-at-the-end">A short break at the end</a><br />
</li>
</ol></li>
<li><a href="#appendix">Appendix</a>
<ol>
<li><a href="#a-highly-accelerated-introduction-to-natural-deduction">A highly-accelerated introduction to natural deduction</a><br />
</li>
<li><a href="#motivation-from-proof-theory-or-the-ice-cold-correspondence">Motivation from proof theory, or the ice-cold correspondence</a><br />
</li>
<li><a href="#computational-significance-of-local-completeness">Computational significance of local completeness</a><br />
</li>
<li><a href="#a-foundation-for-the-resource-management-via-copydiscard">A foundation for the resource management via copy/discard</a><br />
</li>
<li><a href="#can-we-expect-acceptable-performance-from-this-approach">Can we expect acceptable performance from this approach?</a><br />
</li>
<li><a href="#miscellaneous">Miscellaneous</a><br />
</li>
</ol></li>
<li><a href="#afterword">Afterword</a></li>
</ol>
</div>
<h2>Exploiting the power of linearity</h2>
<h3>A language that uses every variable just once</h3>
<p>It would be a straightforward idea to think about a simpler language before we dive into a fully-equipped one. Let's take that way here, too.</p>
<p>We can restrict a programming language in various ways. Here, we add a restriction on the use of variables. More specifically, we consider a programming language like OCaml or Haskell, with the following additional property: every variable is used exactly once. Such a use of a variable is called to be "linear."</p>
<p>Let's call our language L<sup>-</sup>. The next pseudo-code, for example, should be valid in L<sup>-</sup>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>let x := 100 in</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>let y := 1 in</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>add x y</span></code></pre></div>
<p>Note that both of the variables <code>x</code> and <code>y</code> are used linearly. On the other hand, the following pseudo-code should be invalid:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>let x := 100 in</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>add x x</span></code></pre></div>
<p>This is because the variable <code>x</code> is used twice in <code>add x x</code>. Also, the next code:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>let x := 100 in</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>10</span></code></pre></div>
<p>should be invalid since the variable <code>x</code> is not used. Or, assuming that a function <code>increment</code> is defined beforehand, the next pseudo-code should be invalid, again:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>let x := 100 in</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>let y := increment x in</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>add x y</span></code></pre></div>
<p>We can easily check that the variable <code>x</code> is used twice in total (<code>increment x</code> and <code>add x y</code>). The <code>x</code> is already consumed at <code>increment x</code>, and thus we can no longer use it at the point of <code>add x y</code>, so to speak.</p>
<p>Of course, imposing linearity to every variable is really a demanding restriction. We won't be able to write a real program in a language with such a restriction. We will, however, see how this restriction can be bypassed soon, so we don't have to worry about the problem of expressivity now.</p>
<h3>Resource management under linearity</h3>
<p>Here we assume that the language L<sup>-</sup> consists of variables, lambda-abstractions, function applications, and <code>let</code>. We omit arrays like <code>[1, 2, 3]</code>, or integers like <code>100</code> that we see above. This is because what we see below can be applied essentially as it is to those omitted constructs; We omit them so as not to make the text needlessly long.</p>
<p>Let's think about realizing static resource management in our language L<sup>-</sup>. Then we can easily find the following immediate solution.</p>
<p>We allocate memory when and only when we process lambda-abstraction. For example, let's consider the following code:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>let f := λ y. (some computation) in</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>(remaining computation)</span></code></pre></div>
<p>The code should behave as follows:</p>
<ol>
<li>Allocates a piece of memory to express <code>λ y. (some computation)</code>,<br />
</li>
<li>Writes information of the lambda-abstraction (this information is represented by a tuple like <code>(info-1, ..., info-n)</code>, and referred to as "closure"),<br />
</li>
<li>Binds a pointer to the region to <code>f</code>,<br />
</li>
<li>Executes <code>(remaining computation)</code>.<br />
</li>
</ol>
<p>This behavior shouldn't be much different than that of an ordinary language.</p>
<p>On the other hand, we deallocate memory when and only when we process function application. For example, let's consider the following code:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>(some computation) in</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>f a</span></code></pre></div>
<p>The code should behave as follows:</p>
<ol>
<li>Executes <code>(some computation)</code>,<br />
</li>
<li>Extracts <code>info-1</code>, …, <code>info-n</code> from the variable <code>f</code>,<br />
</li>
<li>Deallocates the tuple <code>(info-1, ..., info-n)</code>,<br />
</li>
<li>Calls the appropriate function with <code>a</code> as its argument.</li>
</ol>
<p>The memory management defined above is safe. It also deallocates all the resources that are allocated during program execution. This is thanks to of the linearity of the language; Firstly, by the linearity, every lambda-abstraction is used exactly once. This means, of course, that every lambda-abstraction is used at most once, and at least once. Since they are used (applied) at most once, a lambda abstraction is deallocated at most once. This guarantees the property "a deallocated resource won't be deallocated again." Also, since they are used (applied) at least once, a lambda abstraction is deallocated at least once. This guarantees the property "every lambda abstraction is deallocated."</p>
<p>All in all, the language L<sup>-</sup> can realize static memory management by the interpretation above. The language already has our desired property. In the next section, we look for a way to enhance the expressivity of this language, keeping the charming property intact.</p>
<h3>Smuggling non-linearity</h3>
<p>We need loopholes against linearity, and nothing prevents us from injecting them into our language. Let's add the constants below for any type <code>A</code>:</p>
<ul>
<li><code>copy_A : A -&gt; A * A</code><br />
</li>
<li><code>discard_A : A -&gt; top</code></li>
</ul>
<p>Here, the <code>A * A</code> is the type of a pair that consists of two values of type <code>A</code>. The <code>top</code> is so-called unit type. A little thought makes us realize that these constants can be used to bypass the restriction of linearity. Consider the following invalid code:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>let x := 1 in</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>add x (add x x)</span></code></pre></div>
<p>The code above can be rewritten using those constants:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>let x := 1 in</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>let (x1, tmp) := copy_int x in</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>let (x2, x3) := copy_int tmp in</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>let (add1, add2) := copy_(int-&gt;int-&gt;int) add in</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>add1 x1 (add2 x2 x3)</span></code></pre></div>
<p>The code is now valid as a code in the language L<sup>-</sup>. Or, consider the following:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>let x := 100 in</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>10</span></code></pre></div>
<p>Similarly, this can be rewritten as follows:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>let x := 100 in</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>let () := discard_int x in</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>10</span></code></pre></div>
<p>In both cases, the resulting code uses every variable lineary thanks to <code>copy</code> or <code>discard</code>. More generally, if a variable <code>x</code> of type <code>A</code> is used for n times,</p>
<ul>
<li>if n &lt; 1, we can use <code>discard_A</code> to make the use of <code>x</code> linear.<br />
</li>
<li>if n = 1, the use of <code>x</code> is already linear.<br />
</li>
<li>if n &gt; 1, we can use <code>copy_A</code> to make the use of <code>x</code> linear.</li>
</ul>
<p>This recovers the expressivity that once was diminished by the imposition of linearity. Also, since we didn't touch the behavior of the language, only these constants are peculiar from the viewpoint of resource management. Thus now we just have to consider how these constants can be realized using other language constructs, assuming that it is possible<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<h2>Executing Types</h2>
<p>As quickly mentioned in the preface, we can use types to realize static resource management, or to implement those constants. In this section, firstly we see the basic idea of how to utilize a type for resource management. Next, we see how various types are translated to realize <code>copy_A</code> and <code>discard_A</code> under the idea. Finally, we see how those results of the translation are utilized.</p>
<h3>Finding vocabulary for resource management in the target language</h3>
<p>Let's see the basic idea by an example. Consider we have a term <code>e</code> of type <code>A * B</code>. In this situation, we can expand <code>e</code> as follows, without knowing the internal construction of <code>e</code> is:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>let (x, y) := e in (x, y)</span></code></pre></div>
<p>Such a expansion is often referred to as an η-expansion. This operation keeps the meaning of a term (as long as the <code>e</code> doesn't contain any effects):</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>   let (x, y) := (&quot;foo&quot;, (3, true)) in (x, y)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>~&gt; (&quot;foo&quot;, (3, true))</span></code></pre></div>
<p>Now, the point here is that we can perform this expansion to <code>e</code> by knowing only the type of <code>e</code>. We don't have to care about how <code>e</code> is actually constructed. This means that we can turn the operation of η-expansion for <code>A * B</code> into a function:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>λ z.</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  let (x, y) := z in</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  (x, y)</span></code></pre></div>
<p>The virtue of this function is that it allows us to inspect the internal structure of <code>e</code> by using the variables <code>x</code> and <code>y</code>. It allows us to trace the content of <code>e</code>. Now, using this η-expansion as a reference, let's suppose that we can define a translation <code>Expand(_)</code> that turns a type into a function that traces the terms of the type. <code>Expand(A * B)</code> should be something like this:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>λ z.</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  let (x, y) := z in</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  let x&#39; := Expand(A) x in</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  let y&#39; := Expand(B) y in</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  (x&#39;, y&#39;)</span></code></pre></div>
<p>If we can define this <code>Expand(_)</code> to other types, we should be able to trace every term recursively.</p>
<p>Of course, even if we can define such <code>Expand(_)</code>, it doesn't mean that we can copy/discard resources. It only means that we can now propagate η-expansion to a term, so to speak. The problem of copy/discard is, however, almost solved already. For example, let's suppose that we can define a translation <code>Copy(_)</code> that turns a type into the corresponding "copy" function of the type. Now we can define <code>Copy(A * B)</code> as follows:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>λ z.</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  let (x, y) := z in</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  let (x1, x2) := Copy(A) x in</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  let (y1, y2) := Copy(B) y in</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  ((x1, y1), (x2, y2))</span></code></pre></div>
<p>This function is indeed of type <code>A * B -&gt; (A * B) * (A * B)</code>. Or, let's suppose that we can define a translation <code>Discard(_)</code> that turns a type into the corresponding "discard" function. Again, <code>Discard(A * B)</code> can be defined as follows:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>λ z.</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  let (x, y) := z in</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  let () := Discard(A) x in</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  let () := Discard(B) y in</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  ()</span></code></pre></div>
<p>This function is of type <code>A * B -&gt; top</code>.</p>
<p>After all, the core idea is to implement <code>copy_A</code> and <code>discard_A</code> by extending the functionalized η-expansion into "the power of n". To give such a computational interpretation to types. To translate a type <code>A</code> into the pair <code>(copy_A, discard_A)</code> and extract the required element from this pair and use it to turn a non-linear code into a linear one. The repository that we see in the preface is an implementation of this idea.</p>
<p>Incidentally, in its actual implementation, a type <code>A</code> is translated into not the pair of <code>copy_A</code> and <code>discard_A</code>, but the following 2-ary function <code>exp_A</code>:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>λ flag z.</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  if flag</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  then discard_A z</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  else copy_A z</span></code></pre></div>
<p>This <code>exp_A</code> is used as follows:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>-- to discard x : A</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>let () := exp_A true x in</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>(...)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>-- to copy x : A</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>let (x1, x2) := exp_A false x in</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>(...)</span></code></pre></div>
<p>This is just an implementation-level optimization. This isn't expressivity-related stuff. By adopting this, a type is translated into a closed function, not a pair. Since a closed function is represented as a simple function pointer, it can be copied/discarded just in the same way as an immediate value like an integer. Thus we can copy/discard the result of the translation of a type as if it were an integer. This omits tedious allocations/deallocations that would've been necessary if we had taken the other approach. Our approach is also preferable from the viewpoint of performance. That's why I chose this approach in the actual implementation.</p>
<h3>How types are translated, more concretely</h3>
<p>Here, we see how <code>copy</code> and <code>discard</code> are defined for various types.</p>
<h4>Immediate</h4>
<p>On immediate types like <code>int</code>. We can define <code>copy</code> and <code>discard</code> for them as follows:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>let copy_int :=</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  λ x. (x, x)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>let discard_int :=</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  λ x. ()</span></code></pre></div>
<p>Since the argument of <code>copy_int</code> and <code>discard_int</code> are immediate, it can be copied/discarded without any memory operations. Thus we can use the argument in non-linear manner. The allocating operation for <code>(x, x)</code> is the only memory-related operation in these functions.</p>
<h4>Array</h4>
<p>On array types like <code>int[3]</code> (Here we assume that every value of an array is immediate). We can define <code>copy</code> and <code>discard</code> as follows:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>let copy_int_3 :=</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  λ x.</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    let [a, b, c] := x in</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    ([a, b, c], [a, b, c])</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>let discard_int_3 :=</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>  λ x.</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    let [a, b, c] := x in</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    ()</span></code></pre></div>
<p>That is, we can extract values from <code>x</code> and then construct a new array. Here, the meaning of</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>let [a, b, c] := x in (...)</span></code></pre></div>
<p>is assumed to be something like this:</p>
<ol>
<li>binds all the elements to <code>a</code>, <code>b</code>, and <code>c</code>,<br />
</li>
<li>deallocates the array <code>x</code>.<br />
</li>
</ol>
<p>Thus, the behavior of <code>copy</code> is, for example, as follows:</p>
<ol>
<li>binds all the elements of <code>x</code> to <code>a</code>, <code>b</code>, and <code>c</code><br />
</li>
<li>deallocates <code>x</code><br />
</li>
<li>allocates a piece of memory for <code>[a, b, c]</code> (the first time)<br />
</li>
<li>writes <code>[a, b, c]</code> to the memory region (the first time)<br />
</li>
<li>allocates a piece of memory for <code>[a, b, c]</code> (the second time)<br />
</li>
<li>writes <code>[a, b, c]</code> to the memory region (the second time)<br />
</li>
<li>allocates a piece of memory for <code>([a, b, c], [a, b, c])</code><br />
</li>
<li>writes <code>([a, b, c], [a, b, c])</code> to the memory region<br />
</li>
</ol>
<p>Note that we can copy <code>a</code>, <code>b</code>, and <code>c</code> without any additional operations since they are immediate.</p>
<h4>Type of Types</h4>
<p>The <code>Type</code> in <code>A : Type</code> is also a type, and thus it is something to be translated. It can be, however, treated in the same way as an immediate thanks to the optimization that we've seen. Thus, we can define the <code>copy</code> and <code>discard</code> for the type of types simply as follows:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>let copy_type :=</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  λ x. (x, x)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>let discard_type :=</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  λ x. ()</span></code></pre></div>
<h4>Function</h4>
<p>On function types like <code>int -&gt; bool</code>. This is a little complicated. You might want to skip this if you just want to catch the general drift. Anyway, we need to see how a lambda-abstraction is translated to explain the behavior of a function type. Consider the following code.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>let f :=</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  let b := true in</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>  let y := 10 in</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>  λ x. x + (as-int b) + y in</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>(...)</span></code></pre></div>
<p>Here, <code>as-int</code> is a function that (for example) translates <code>true</code> to <code>1</code>, and <code>false</code> to <code>0</code>, respectively.</p>
<p>The code above contains a lambda abstraction <code>λ x. x + (as-int b) + y</code> that has <code>b : bool</code> and <code>y : int</code> as its free variables. In an ordinary programming language, such a lambda-abstraction is translated into the following pair:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>((b, y),</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  λ (x, env).</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    let (b, y) := env in</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    x + (as-int b) + y)</span></code></pre></div>
<p>That is, a pair of the following form:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>({the set of all the free variables},</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a> λ ({the original arguments}, env).</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>   let (the names of the free variables) := env in</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>   {the original code})</span></code></pre></div>
<p>This translation is referred to as closure conversion. In our system, we extend this procedure; We translate the lambda abstraction into the following 3-tuple:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>(bool * int,</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a> (b, y),</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>  λ (x, env).</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    let (b, y) := env in</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    x + (as-int b) + y)</span></code></pre></div>
<p>That is, we attach the type information of the free variables<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. With this information, we can easily copy/discard a closure. Indeed, for every element of a closure,</p>
<ul>
<li><code>bool * int</code> can be copied/discarded as an immediate since it is a type.<br />
</li>
<li><code>(b, y)</code> can be copied/discarded using <code>bool * int</code>.<br />
</li>
<li>the third element can be copied/discarded as an immediate since it is a function pointer to a closed function.<br />
</li>
</ul>
<p>This realizes the <code>copy</code> and <code>discard</code> of a closure<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>After all, the <code>copy</code> and <code>discard</code> for a function type like <code>int -&gt; bool</code> are defined as follows:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>let copy_closure :=</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  λ cls.</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    let (env_type, env, func) := cls in</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    let (env1, env2) := env_type false env in</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    ((env_type, env1, func), (env_type, env2, func))</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>let discard_closure :=</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>  λ cls.</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    let (env_type, env, func) := cls in</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    let () := env_type true env in</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    ()</span></code></pre></div>
<p>Here, the behavior of</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>let (x1, ..., xn) := x in (...)</span></code></pre></div>
<p>is assumed to be something like:</p>
<ol>
<li>binds all the elements of <code>x</code> to <code>x1</code>, …, <code>xn</code>,<br />
</li>
<li>deallocates <code>x</code></li>
</ol>
<h3>How translated types are utilized</h3>
<p>Finally, let's see how these results of the translation are utilized to linearize given code. Consider the following function:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>let to-pair :=</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  λ (A : Type) (x : A). (x, x)</span></code></pre></div>
<p>This function <code>to-pair</code> is something that is used in the following way:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>to-pair int         3              # ~&gt; (3, 3)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>to-pair string      &quot;hello&quot;        # ~&gt; (&quot;hello&quot;, &quot;hello&quot;)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>to-pair (bool * top) (false, unit) # ~&gt; ((false, unit), (false, unit))</span></code></pre></div>
<p><code>to-pair</code> is a polymorphic function that creates the pair of the given argument.</p>
<p>As you can see, the variable <code>x</code> is used twice in the definition of <code>to-pair</code>. This non-linear <code>x</code> is linearized using <code>A</code> essentially as follows:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>let to-pair :=</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>  λ A x.</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    let (x1, x2) := A false x in</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    (x1, x2)</span></code></pre></div>
<p>The function <code>to-pair</code> receives various kinds of values at the position of <code>x</code>. It can, however, copy the value <code>x</code> since the accompanying argument <code>A</code> necessarily contains the required information to copy the value. The same applies to <code>discard</code>.</p>
<h3>A short break at the end</h3>
<p>The above concludes the main part of this article. We see how static memory management is realized by executing types. In a highly sketchy manner, admittedly.</p>
<p>I believe that you can now guess why I chose to use dependent type theory in this attempt; The theory just simplifies the implementation since a type in the theory occurs in a program just in the same way as a term.</p>
<p>Incidentally, I've seen a lot of introductory articles that support the usefulness of dependent type theory by emphasizing the possibility of length-annotated array types. Such a type can be used to realize array accessing in a safe way. Yes, that's completely true. At the same time, however, I'd like to emphasize another virtue of such a theory here. That is, it makes the language more integrated: Both of the type-level abstraction (i.e. <code>forall</code>) and the term-level abstraction (i.e. function) are represented by the same syntax construct (i.e. <code>λ</code>). We don't need an additional concept to, for example, define a type. This property can be something that appeals to those who seek for a theoretical virtue.</p>
<p>Also, I'll add a note here. I'm writing this article, thinking that the method that I've shown in this article is new (to some degree). As a general rule, however, there is often a more thoughtful person who has already investigated the very thing that I think is new, and the investigation is often more sophisticated than mine. If it is the case, I hope that this article works as a useful annotation to the preceding research.</p>
<p>Anyway, the main part ends here. Reading the additional contents below should make the main part more attractive, like a fighting game with a basic understanding of the theory behind it. Let's go ahead.</p>
<h2>Appendix</h2>
<h3>A highly-accelerated introduction to natural deduction</h3>
<p>I tried to omit this section at first, but it turned out to be essential for the explanation. That's why I write a highly-accelerated introduction to the natural deduction. If you want to read a more thorough introductory article, I think you can refer to <a href="https://www.cs.cmu.edu/~fp/courses/15317-f09/schedule.html">the lecture notes by Pfenning</a>. Many thanks to the author and Carnegie Mellon University.</p>
<h4>Encounter with propositional logic</h4>
<p>Let's fix a set of distinct symbols. We call an element of this set a propositional variable. We also assume that there are infinitary many propositional variables (the number of them is assumed to be exactly the same as that of the natural numbers). We then define "proposition" as follows:</p>
<ol>
<li>If <span class="math inline">\( \alpha \)</span> is a propositional variable, then <span class="math inline">\( \alpha \)</span> is a proposition.<br />
</li>
<li>If <span class="math inline">\( A, B \)</span> are propositions, then <span class="math inline">\( A \to B \)</span> is a proposition.<br />
</li>
<li>No other syntactic construct is a proposition.<br />
</li>
</ol>
<p>For example, if <span class="math inline">\( P \)</span>, <span class="math inline">\( Q \)</span>, and <span class="math inline">\( R \)</span> are propositional variables, then all of <span class="math inline">\( P \)</span>, <span class="math inline">\( P \to Q \)</span>, <span class="math inline">\( P \to (Q \to R) \)</span> and <span class="math inline">\( (P \to P) \to R \)</span> are propositions.</p>
<p>You may now think that "What are those parentheses in <span class="math inline">\( P \to (Q \to R) \)</span>?" It's actually not that important, but I'll answer this question here just in case. These parentheses are required because, if we simply write <span class="math inline">\( P \to Q \to R \)</span>, we don't know how to tell if it represents this tree:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>    →</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>   / \</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>  →   R</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a> / \</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>P   Q</span></code></pre></div>
<p>or this tree:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>  →</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a> / \</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>P   →</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>   / \</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>  Q   R</span></code></pre></div>
<p>The parentheses here are meta-level entities that allow us to represent a tree-structure in a sentence. Indeed, we don't need them if we write a tree structure every time we need it, but it'll be tedious and space-demanding. To summarize:</p>
<ol>
<li>The "<span class="math inline">\( A \to B \)</span>" in "<span class="math inline">\( A \to B \)</span> is a proposition" is not a character sequence but a tree structure<br />
</li>
<li>Writing a tree structure every time is troublesome<br />
</li>
<li>By the way, we can use parentheses to represent a tree structure in a sentence<br />
</li>
<li>Then let's use it as a useful abbreviation<br />
</li>
</ol>
<p>Using parentheses is a simple trick to represent a tree structure.</p>
<p>Also, one might think that the last condition "No other syntactic construct is a proposition" is peculiar. This is, again, not so complex. This is just to say "no" when we're asked like, for example, "Then, is <span class="math inline">\( \uparrow \uparrow \downarrow \downarrow \leftarrow \to \leftarrow \to  A B\)</span> a proposition?" Without the last condition, we don't know what is not a proposition.</p>
<p>Next, we define "quasi-context" as follows.</p>
<ol>
<li><span class="math inline">\( \cdot \)</span> is a quasi-context.<br />
</li>
<li>If <span class="math inline">\( \Gamma \)</span> is a quasi-context and <span class="math inline">\( A \)</span> is a proposition, then <span class="math inline">\( \Gamma, A \)</span> is a quasi-context.<br />
</li>
<li>No other syntactic construct is a quasi-context.<br />
</li>
</ol>
<p>In short, a quasi-context is a list of propositions. Something like <span class="math inline">\( \cdot, A, B, C \)</span>. Or, more explicitly, a tree structure like:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>      ,</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>     / \</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    ,   C</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>   / \</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>  ,   B</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a> / \</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>.   A</span></code></pre></div>
<p>We don't need any parentheses this time because we don't have a tree of the following form, for example:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>    ,</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>   / \</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>  ,   C</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a> / \</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>A   ,</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>   / \</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>  .   B</span></code></pre></div>
<p>In other words, that's because it suffices to say that "it is a quasi-context" to specify the tree structure of <span class="math inline">\( \cdot, A, B, C \)</span>.</p>
<p>We call <span class="math inline">\( \cdot \)</span> an empty quasi-context. As you can see from the example above, a non-empty quasi-context is of the form <span class="math inline">\( \cdot, A_1, \ldots, A_n \)</span>. Such a quasi-context is often written as <span class="math inline">\( A_1, \ldots, A_n \)</span>, omitting the <span class="math inline">\( \cdot \)</span>.</p>
<p>We define a context to be a quasi-context without order. For example, <span class="math inline">\( A, B, C, C \)</span> and <span class="math inline">\( C, B, A, C \)</span> are different when seen as quasi-contexts, but the same when seen as contexts.</p>
<p>Let's define a judgment as follows.</p>
<ol>
<li>If <span class="math inline">\( \Gamma \)</span> is a context and <span class="math inline">\( A \)</span> is a proposition, then <span class="math inline">\( \Gamma \vdash A \)</span> is a judgement.<br />
</li>
<li>No other syntactic construct is a judgment.<br />
</li>
</ol>
<p>For example, all of <span class="math inline">\( A \vdash A \)</span>, <span class="math inline">\( C \vdash A \to (B \to B) \)</span>, and <span class="math inline">\( \Gamma \vdash A \)</span> are judgement.</p>
<p>Our "judgment" is, despite its suggestive name, currently just a syntactic construct with a certain pattern. Just a tree structure with a mysterious name. We'd like to reach the point where we can interpret <span class="math inline">\( \Gamma \vdash A \)</span> as "Assuming <span class="math inline">\( \Gamma \)</span>, <span class="math inline">\( A \)</span> is true." We don't, however, have any frameworks that allow us to interpret our judgments.</p>
<p>So let's construct such a framework. A framework that allows us to say "This judgment is correct," or "not correct." We're going to, roughly speaking, define a framework to talk about the meaning of a judgment.</p>
<p>Generally speaking, there are basically two approaches to define the meaning of a symbol.</p>
<ol>
<li>The internal approach. In this approach, we define what a symbol refers to. This approach relates the symbol "that apple" to that red object on that table. This is an approach that focuses on the internals of a symbol, so to speak. If the referred object (= meaning) is defined, we can say that the referred object (= meaning) is not correct when, for example, the symbol "that apple" is used to refer to the Tale of Genji.<br />
</li>
<li>The external approach. In this approach, we define how a symbol is used. The approach relates the symbol "that apple" to the use of it like "to turn the attention of the listener to that red object on that table." This is an approach that focuses on the behavior of a symbol, so to speak. If the use (= meaning) is defined, we can say that the use (= meaning) is not correct when, for example, the listener starts headbanging as soon as the person perceived the utterance "that apple."</li>
</ol>
<p>We'll take the latter approach here. Using a few rules, we'll define how a symbol that we named a "judgment" is used. Such a rule is called as an inference rule.</p>
<p>An inference rule is represented in the following form:</p>
<p><span class="math display">\[
\require{bussproofs}
\begin{prooftree}
  \AxiomC{\( \mathcal{J}_1 \hspace{1em} \ldots \hspace{1em} \mathcal{J}_n \)}
  \RightLabel{\( \mathsf{(name)} \)}
  \UnaryInfC{\( \mathcal{J} \)}
\end{prooftree}
\]</span></p>
<p>The <span class="math inline">\( \mathcal{J}_i \)</span>s above the horizontal line are the judgments. They are the premises of this rule. The inference rule allows us to write the horizontal line and the additional judgment <span class="math inline">\( \mathcal{J} \)</span> when all the premises are there. The <span class="math inline">\( \mathsf{(name)} \)</span> is the name of the rule.</p>
<p>Let's see actual rules. The first one is the rule of variable:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{}
  \RightLabel{\( (\mathsf{var}) \)}
  \UnaryInfC{\( \Gamma, A \vdash A \)}
\end{prooftree}
\]</span></p>
<p>This is an inference rule that doesn't need any premises. That's why there is nothing above the horizontal line. Intuitively, this rule can be read like "When <span class="math inline">\( A \)</span> is assumed, this <span class="math inline">\( A \)</span> implies <span class="math inline">\( A \)</span>. The same holds when we put additional assumptions <span class="math inline">\( \Gamma \)</span>." Or, more specifically, by adopting the inference rule above, the "<span class="math inline">\( \vdash \)</span>" turns into something that can be compared to "implies" in our language.</p>
<p>Let's see some examples. All of below are correct applications of the rule <span class="math inline">\( \mathsf{(var)} \)</span>:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( B, A \vdash A \)}
\end{prooftree}
\hspace{1em}
\begin{prooftree}
  \AxiomC{}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( A \vdash A \)}
\end{prooftree}
\hspace{1em}
\begin{prooftree}
  \AxiomC{}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( A, B, C, D \vdash A \)}
\end{prooftree}
\]</span></p>
<p>On the other hand, all of below are incorrect applications of the rule <span class="math inline">\( \mathsf{(var)} \)</span>:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( A, C \vdash B \)}
\end{prooftree}
\hspace{1em}
\begin{prooftree}
  \AxiomC{}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( A \vdash A \to A \)}
\end{prooftree}
\hspace{1em}
\begin{prooftree}
  \AxiomC{}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( \cdot \vdash A \)}
\end{prooftree}
\]</span></p>
<p>Let's move on to the next rule. The next rule is something that embeds the meaning of "<span class="math inline">\( \vdash \)</span>" to the proposion-level construct "<span class="math inline">\( \to \)</span>":</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \Gamma, A \vdash B \)}
  \RightLabel{\( (\to_{\mathsf{i}}) \)}
  \UnaryInfC{\( \Gamma \vdash A \to B \)}
\end{prooftree}
\]</span></p>
<p>Intuitively, this is something that should be read as: "When '<span class="math inline">\( \Gamma \)</span> and <span class="math inline">\( A \)</span> implies <span class="math inline">\( B \)</span>' is correct, '<span class="math inline">\( \Gamma \)</span> implies <span class="math inline">\( A \to B \)</span>' is correct." We've just defined the meaning of "<span class="math inline">\( \vdash \)</span>" to be implication — or something that can be compared to it at least — using the rule <span class="math inline">\( \mathsf{(var)} \)</span>. In turn, this inference rule <span class="math inline">\( (\to_{\mathsf{i}}) \)</span> is something that sends the judgement-level symbol "<span class="math inline">\( \vdash \)</span>" into the proposition-level symbol "<span class="math inline">\( \to \)</span>".</p>
<p>The rule above is something that generates a new proposition that contains "<span class="math inline">\( \to \)</span>". In other words, this rule defines when we can say certain proposition. Such an inference rule is said to be an introduction rule. Conversely, a rule that defines what can be said from a proposition is called an elimination rule. The elimination rule of "<span class="math inline">\( \to \)</span>" is as follows:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \Gamma \vdash A \to B \)}
  \AxiomC{\( \Gamma \vdash A \)}
  \RightLabel{\( (\to_{\mathsf{e}}) \)}
  \BinaryInfC{\( \Gamma \vdash B \)}
\end{prooftree}
\]</span></p>
<p>This is something that defines how to use an implication "<span class="math inline">\( \to \)</span>". This is a rule that allows us to derive "<span class="math inline">\( B \)</span>" when we know "<span class="math inline">\( A \)</span> implies <span class="math inline">\( B \)</span>" and "<span class="math inline">\( A \)</span>". I believe that there's no surprise here.</p>
<p>We take the three rules above, that is,</p>
<ol>
<li>The rule of variable<br />
</li>
<li>The introduction rule of implication<br />
</li>
<li>The elimination rule of implication<br />
</li>
</ol>
<p>as the inference rules of our logical system. We can easily add, for example, AND, OR, or whatever.</p>
<p>We can generate, for example, the following pattern by applying the rules above repeatedly:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \)}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( B, B, A \vdash A \)}
  \RightLabel{\( (\to_\mathsf{i}) \)}
  \UnaryInfC{\( B, B \vdash A \to A \)}
  \RightLabel{\( (\to_\mathsf{i}) \)}
  \UnaryInfC{\( B \vdash B \to (A \to A) \)}
  \AxiomC{\( \)}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( B \vdash B \)}
  \RightLabel{\( (\to_\mathsf{e}) \)}
  \BinaryInfC{\( B \vdash A \to A \)}
  \RightLabel{\( (\to_\mathsf{i}) \)}
  \UnaryInfC{\( \cdot \vdash B \to (A \to A) \)}
\end{prooftree}
\]</span></p>
<p>Such a generated tree is said to be a proof tree, or simply a proof. A proof of <span class="math inline">\( \cdot \vdash B \to (A \to A) \)</span>, in this case.</p>
<h4>Detours in a proof tree</h4>
<p>We can derive a judgement <span class="math inline">\( \Gamma \vdash A \)</span> in various ways. For example, consider proving <span class="math inline">\( \cdot \vdash A \to A \)</span>. Of course, we have the following straightforward proof:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\(  \)}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( A \vdash A \)}
  \RightLabel{\( (\to_\mathsf{i}) \)}
  \UnaryInfC{\( \cdot \vdash A \to A \)}
\end{prooftree}
\]</span></p>
<p>On the other hand, we also have the following redundant proof:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\(  \)}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( A, A \vdash A \)}
  \RightLabel{\( (\to_\mathsf{i}) \)}
  \UnaryInfC{\( A \vdash A \to A \)}
  \AxiomC{\(  \)}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( A \vdash A \)}
  \RightLabel{\( (\to_\mathsf{e}) \)}
  \BinaryInfC{\( A \vdash A \)}
  \RightLabel{\( (\to_\mathsf{i}) \)}
  \UnaryInfC{\( \cdot \vdash A \to A \)}
\end{prooftree}
\]</span></p>
<p>The proof above derives the same <span class="math inline">\( \cdot \vdash A \to A \)</span>. The proof tree is, nevertheless, unnecessarily complex.</p>
<p>Where does this complexity come from? Why is the proof tree above unnecessarily big? — That's because the proof tree contains a "detour." Specifically, the "detour" here is the following part:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( A, A \vdash A \)}
  \RightLabel{\( (\to_\mathsf{i}) \)}
  \UnaryInfC{\( A \vdash A \to A \)}
  \AxiomC{\( A \vdash A \)}
  \RightLabel{\( (\to_\mathsf{e}) \)}
  \BinaryInfC{\( A \vdash A \)}
\end{prooftree}
\]</span></p>
<p>This is schematically a proof that introduces the logical connective "<span class="math inline">\( \to \)</span>," and then immediately eliminates the connective. Introduction followed by immediate elimination. But doesn't it mean that we didn't have to introduce the connective after all? In this sense, the above is a "detour." More generally, such a "detour" is of the following form:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \mathcal{H}_1 \)}
  \UnaryInfC{\( \Gamma, A \vdash B \)}
  \RightLabel{\( (\to_\mathsf{i}) \)}
  \UnaryInfC{\( \Gamma \vdash A \to B \)}
  \AxiomC{\( \mathcal{H}_2 \)}
  \UnaryInfC{\( \Gamma \vdash A \)}
  \RightLabel{\( (\to_\mathsf{e}) \)}
  \BinaryInfC{\( \Gamma \vdash B \)}
\end{prooftree}
\]</span></p>
<p>That is, a "detour" is a pattern "introduction followed by immediate elimination." Such a "detour" is often called as a redex (Here, the symbols <span class="math inline">\( \mathcal{H}_1 \)</span> and <span class="math inline">\( \mathcal{H}_2 \)</span> represent the upper proof trees).</p>
<p>Let's take five minutes or so and gaze at the redex above. Then we'll see that we can construct a proof tree of <span class="math inline">\( \Gamma \vdash B \)</span> that doesn't contain the redex. The construction can be done as follows. Firstly, focus on the following part:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \mathcal{H}_1 \)}
  \UnaryInfC{\( \Gamma, A \vdash B \)}
\end{prooftree}
\]</span></p>
<p>Now, suppose that the <span class="math inline">\( A \)</span> in <span class="math inline">\( \Gamma, A \vdash B \)</span> is used somewhere in <span class="math inline">\( \mathcal{H}_1 \)</span>. In such a situation, we replace this <span class="math inline">\( A \)</span> by the <span class="math inline">\( A \)</span> in the following proof tree:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \mathcal{H}_2 \)}
  \UnaryInfC{\( \Gamma \vdash A \)}
\end{prooftree}
\]</span></p>
<p>By this modification, we now don't have to use the <span class="math inline">\( A \)</span> of <span class="math inline">\( \Gamma, A \vdash B \)</span>. This means that we can prove <span class="math inline">\( B \)</span> without using the <span class="math inline">\( A \)</span> of <span class="math inline">\( \Gamma, A \vdash B \)</span>. That is, if we define <span class="math inline">\( \mathcal{H&#39;}_1 \)</span> to be the proof tree obtained from <span class="math inline">\( \mathcal{H}_1 \)</span> by</p>
<ol>
<li>using <span class="math inline">\( A \)</span> not from the context but from <span class="math inline">\( \mathcal{H}_2 \)</span>, and<br />
</li>
<li>removing the <span class="math inline">\( A \)</span> in the context,</li>
</ol>
<p>then we can derive the following tree:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \mathcal{H&#39;}_1 \)}
  \UnaryInfC{\( \Gamma \vdash B \)}
\end{prooftree}
\]</span></p>
<p>This rewriting operation can be summarized as follows:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \mathcal{H}_1 \)}
  \UnaryInfC{\( \Gamma, A \vdash B \)}
  \RightLabel{\( (\to_\mathsf{i}) \)}
  \UnaryInfC{\( \Gamma \vdash A \to B \)}
  \AxiomC{\( \mathcal{H}_2 \)}
  \UnaryInfC{\( \Gamma \vdash A \)}
  \RightLabel{\( (\to_\mathsf{e}) \)}
  \BinaryInfC{\( \Gamma \vdash B \)}
\end{prooftree}
\hspace{3em}
\leadsto
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( {\mathcal{H&#39;}_1} \)}
  \UnaryInfC{\( \Gamma \vdash B \)}
\end{prooftree}
\]</span></p>
<p>Such a rewriting operation that resolves a redex is called a reduction. The process of obtaining a proof tree without any redex by reducing the given tree repeatedly is called normalization.</p>
<h4>Normalizing a proof tree / executing a program</h4>
<p>In the discussion above, we've denoted a proof tree by a symbol <span class="math inline">\( \mathcal{H} \)</span>. Here, we consider keeping this information in a more local way. We consider keeping the "log" information of a proof every time we apply an inference rule. The log information must be something that can be used when we want to recover the proof of a judgment under consideration. Firstly, let's see the inference rule of a variable:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\(  \)}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( \Gamma, A \vdash A \)}
\end{prooftree}
\]</span></p>
<p>We'd like to add a log information for this inference rule. The log information must be something that be used to tell which <span class="math inline">\( A \)</span> in the context is actually used in the following application:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\(  \)}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( A, A \vdash A \)}
\end{prooftree}
\]</span></p>
<p>Thus we have to give a name to each proposition in the context. More specifically, we'll do as follows. Firstly, take a set that is exactly as big as the set of natural numbers. Let's call this the variable set. We also call an element of this set a variable. Using this, we extend the definition of a quasi-context as follows:</p>
<ol>
<li><span class="math inline">\( \cdot \)</span> is a quasi-context.<br />
</li>
<li>If <span class="math inline">\( \Gamma \)</span> is a quasi-context and <span class="math inline">\( x \)</span> is a varible and <span class="math inline">\( A \)</span> is a proposition, then <span class="math inline">\( \Gamma, x : A \)</span> is a quasi-context.<br />
</li>
<li>No other syntactic construct is a quasi-context.</li>
</ol>
<p>We also define a "proof term" as follows. We're going to use this to keep track of a proof.</p>
<ol>
<li>If <span class="math inline">\( x \)</span> is a variable, then <span class="math inline">\( x \)</span> is a proof term.<br />
</li>
<li>If <span class="math inline">\( x \)</span> is a variable and <span class="math inline">\( e \)</span> is a proof term, then <span class="math inline">\( \lambda x. e \)</span> is a proof term.<br />
</li>
<li>If <span class="math inline">\( e_1 \)</span> and <span class="math inline">\( e_2 \)</span> are proof terms, then <span class="math inline">\( e_1 \mathbin{@} e_2 \)</span> is a proof term.<br />
</li>
<li>No other syntactic construct is a proof-term.</li>
</ol>
<p>Using this "proof term," we extend the definition of a judgment as follows:</p>
<ol>
<li>If <span class="math inline">\( \Gamma \)</span> is a context and <span class="math inline">\( e \)</span> is a proof term and <span class="math inline">\( A \)</span> is a proposition, then <span class="math inline">\( \Gamma \vdash e : A \)</span> is a judgement.<br />
</li>
<li>No other syntactic construct is a judgment.</li>
</ol>
<p>Now we're ready to extend the rule <span class="math inline">\( \mathsf{(var)} \)</span>. It would me more illuminating to show how the example of <span class="math inline">\( A, A \vdash A \)</span> changes:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\(  \)}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( x : A, y : A \vdash y : A \)}
\end{prooftree}
\]</span></p>
<p>Now that each proposition in the context has a name like <span class="math inline">\( x \)</span> or <span class="math inline">\( y \)</span>, we can keep the information that shows the "active" proposition in the application of the rule <span class="math inline">\( \mathsf{(var)} \)</span>. As a inference rule, the <span class="math inline">\( \mathsf{(var)} \)</span> is extended as follows:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\(  \)}
  \RightLabel{\( \mathsf{(var)} \)}
  \UnaryInfC{\( \Gamma, x : A \vdash x : A \)}
\end{prooftree}
\]</span></p>
<p>The log information of the derivation of a judgment is saved in <span class="math inline">\( e \)</span> of <span class="math inline">\( \Gamma \vdash e : A \)</span>.</p>
<p>Let's move on to the introduction rule of "<span class="math inline">\( \to \)</span>". This is extended as follows:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \Gamma, x : A \vdash e : B \)}
  \RightLabel{\( (\to_{\mathsf{i}}) \)}
  \UnaryInfC{\( \Gamma \vdash \lambda x. e : A \to B \)}
\end{prooftree}
\]</span></p>
<p>The premise of the introduction rule of "<span class="math inline">\( \to \)</span>" is now turned into <span class="math inline">\( \Gamma, x : A \vdash e : B \)</span>. This is just because the definition of a judgment is extended. No surprises. Also, we now have a really connotative proof term <span class="math inline">\( \lambda x. e \)</span> in the conclusion. This is, however, just a term that keeps track of a fact that we applied the extended introduction rule of "<span class="math inline">\( \to \)</span>," focusing on the variable <span class="math inline">\( x \)</span>. This is just a log of a proof. Such an extension is an automatic process; we don't need any creativity here.</p>
<p>Finally, let's move on to the elimination rule of "<span class="math inline">\( \to \)</span>". This is extended as follows:</p>
<p><span class="math display">\[
\newcommand{\app}[2]{#1 \mathbin{@} #2}
\begin{prooftree}
  \AxiomC{\( \Gamma \vdash e_1 : A \to B \)}
  \AxiomC{\( \Gamma \vdash e_2 : A \)}
  \RightLabel{\( (\to_{\mathsf{e}}) \)}
  \BinaryInfC{\( \Gamma \vdash \app{e_1}{e_2} : B \)}
\end{prooftree}
\]</span></p>
<p>Again, we simply added the required proof terms to the rule. No surprises.</p>
<p>Now, let's add proof terms to our detours that we saw above. It generates the following proof tree:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \mathcal{H}_1 \)}
  \UnaryInfC{\( \Gamma, x : A \vdash e_1 : B \)}
  \RightLabel{\( (\to_\mathsf{i}) \)}
  \UnaryInfC{\( \Gamma \vdash \lambda x. e_1 :  A \to B \)}
  \AxiomC{\( \mathcal{H}_2 \)}
  \UnaryInfC{\( \Gamma \vdash e_2 : A \)}
  \RightLabel{\( (\to_\mathsf{e}) \)}
  \BinaryInfC{\( \Gamma \vdash \app{(\lambda x. e_1)}{e_2} : B \)}
\end{prooftree}
\]</span></p>
<p>It looks like, well, something. Let's continue this line pretending ignorance. Remember the operation of resolving a redex. It is, after all, the operation of replacing the use of <span class="math inline">\( x : A \)</span> by <span class="math inline">\( e_2 : A \)</span>. This means that the resulting proof term is the term that can be obtained by replacing all the <span class="math inline">\( x \)</span> in <span class="math inline">\( e_1 \)</span> by <span class="math inline">\( e_2 \)</span>. That is to say, the rewriting operation is summarized as follows:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \mathcal{H}_1 \)}
  \UnaryInfC{\( \Gamma, x : A \vdash e_1 : B \)}
  \RightLabel{\( (\to_\mathsf{i}) \)}
  \UnaryInfC{\( \Gamma \vdash \lambda x. e_1 :  A \to B \)}
  \AxiomC{\( \mathcal{H}_2 \)}
  \UnaryInfC{\( \Gamma \vdash e_2 : A \)}
  \RightLabel{\( (\to_\mathsf{e}) \)}
  \BinaryInfC{\( \Gamma \vdash \app{(\lambda x. e_1)}{e_2} : B \)}
\end{prooftree}
\hspace{3em}
\leadsto
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( {\mathcal{H&#39;}_1} \)}
  \UnaryInfC{\( \Gamma \vdash e_1 \{x := e_2\} : B \)}
\end{prooftree}
\]</span></p>
<p>Here, the <span class="math inline">\( \{x := e_2\} \)</span> is the operation of substitution that replaces all the occurences of the variable <span class="math inline">\( x \)</span> by the term <span class="math inline">\( e_2 \)</span>. We won't go into its rigorous definition here; It's something that translates <span class="math inline">\( x + y + x \)</span> into <span class="math inline">\( e_2 + y + e_2 \)</span>. I believe that your wisdom can imagine its basic behavior.</p>
<p>Anyway, finally, by focusing on the behavior of the proof terms in the reduction above, we obtain the following (hopefully familiar) reduction rule:</p>
<p><span class="math display">\[
  \app{(\lambda x. e_1)}{e_2} \leadsto e_1 \{x := e_2\}
\]</span></p>
<p>Starting from investigating natural deduction, we've reached to the viewpoint of lambda calculus (or programming). From this viewpoint, a judgement <span class="math inline">\( x_1 : A_1, \ldots, x_n : A_n \vdash e : A \)</span> is now read as: "with free variables <span class="math inline">\( x_1 : A_1, \ldots, x_n : A_n \)</span>, the program <span class="math inline">\( e \)</span> is of type <span class="math inline">\( A \)</span>." Especially, a proposition is now interpreted as a type. This story continues like, for example, "resolving a detour in a proof tree is executing a program," or "how we resolve detours in a proof tree corresponds to how we execute a program (like call-by-value, call-by-name)," etc.</p>
<p>This relation between a proof and a program is often referred to as the Curry-Howard correspondence. Our talk on a proof is always-already reinterpretable as a talk on a program, and vice versa.</p>
<p>This correspondence is enlightening and exciting. By extending the proof-side discussion to be able to, for example, represent something like "<span class="math inline">\( P \)</span> is necessarily true," we can obtain <a href="https://www.cs.cmu.edu/~fp/papers/popl96.pdf">the concept of staged computation (something like the quasiquote in lisp)</a> by reinterpreting necessity in the program-side.</p>
<h3>Motivation from proof theory, or the ice-cold correspondence</h3>
<h4>The ice-cold correspondence</h4>
<p>Well, this is the end of an ordinary introduction of the Curry-Howard correspondence. In this article, however, the story doesn't end here. Hey, I was fairly impressed when I first know the correspondence. That's why I tried to create a programming language that utilizes the correspondence to the maximum degree possible. A programming language in which any computational concepts in the language are supported by some proof-theoretic concepts.</p>
<p>A variable can be reinterpreted easily. The introduction rule of implication is easy. The elimination rule is also easy. This and that logical connectives can be added in a straightforward manner. A fixed point operator can be easily added, which makes the language Turing complete. Polymorphism can be realized by extending the logical system to a weak variant of predicate logic. Type inference can be implemented using the well-known method. "Okay, everything seems to be all right," and this is how I reached to the question: "—But how can I manage memory in this language?"</p>
<p>Then I investigated existing languages that have proof-like / lambda-like flavor. They seem to use GC (OCaml, Haskel, F#, Idris, Coq, Agda, Lean). Fair enough. All of them are great in their own way, of course. At the same time, however, it is also a fact that they didn't satisfy my current curiosity; I failed to find one that manages memory in a proof-theoretic way.</p>
<p>All in all, yes, the Curry-Howard correspondence is something that allows us to compare a logical system with an idealized programming language. What I found at the time was that, when considering a real programming language, the aspect of memory management was dealt with as something that should be resolved at the implementation level. It seemed to be recognized as an irregular part of a real programming language, a part that strays from the pure, ice-cold correspondence.</p>
<h4>What about the region-based approach?</h4>
<p>What we'll find when we continue surveying existing works on memory management is the region-based approach. The approach is something that computes the information that is required to realize static memory management by adding annotations to the type system. A great approach. It indeed realizes static memory management. It can, for example, statically detect a wrong use of memory like free-after-free.</p>
<p>However, I was too greedy to accept the approach as an answer to the question. I didn't want to add annotations that don't live in the intuitionistic logic. I didn't want to add non-proof-theoretic, implementation-oriented constructs. I wanted to find the vocabulary for memory management <strong>in</strong> the usual, our familiar natural deduction. I wanted to retain the scheme of "thinking about proofs is always-already thinking about programs" when we thought about memory. That's why I couldn't simply accept the region-based approach, even if the approach is indisputably brilliant under other situations.</p>
<p>I think we should visit the method called "region-inference" here. This is an extension of the ordinary type inference that infers not only type information but also region information — information that can be used to realize static memory management. Using this method, for example, a compiler for Standard ML (A language specification that is similar to OCaml) with static memory management is <a href="https://sourceforge.net/projects/mlkit/">developed</a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>What this implies is that we can realize static memory management for a program that is written in the range of the intuitionistic logic. So isn't this an answer? A language with region-based memory management and region inference. Isn't this the answer that I've been looking for?</p>
<p>The response to this starts by considering the following program in which the type annotation of the lambda abstraction is omitted:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>λx. (not x, 10)</span></code></pre></div>
<p>When inferring the type of the code above, the compiler would generate a metavariable <code>?M</code> that stands for the type of <code>x</code>. That is, the compiler would generate a term like this:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>λ(x : ?M). (not x, 10)</span></code></pre></div>
<p>Then the compiler generates constraints like <code>bool = ?M</code>, using the known type information like <code>not : bool -&gt; bool</code>. The generated constraints are in turn resolved, resulting in a substitution like <code>?M := bool</code>. This substitution is applied to the term above, resulting in the following term:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>λ(x : bool). (not x, 10)</span></code></pre></div>
<p>Thus we now know that the original program is actually an abbreviation of this fully-elaborated program. The point here is that the metavariable <code>?M</code> is inserted in the way above just because the type inference algorithm is defined to do things in that way. If we, for example, want to obtain the number of times that a variable used, the compiler would generate a term like this:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>λ(x : &lt;?M, ?n&gt;). (not x, 10)</span></code></pre></div>
<p>And this term is elaborated into:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>λ(x : &lt;bool, 1&gt;). (not x, 10)</span></code></pre></div>
<p>This means that the original program is actually an abbreviation of this term, in this case. In short, the fully-elaborated form of a program is relative to the type inference algorithm.</p>
<p>Now, region inference is a variant of type inference. This means that a program written in a language with region inference is elaborated in the context of region inference. A program in such a language will be something that can contain abbreviations of not only types but also regions, just like the metavariable <code>?n</code> in the example above. The situation is something like: "We can technically write information on region explicitly, but all of them are accidentally abbreviated this time."</p>
<p>This sums up to the following conclusion: in a language with region inference, even if we might be able to write a program that seems to be closed in the intuitionistic logic, it is actually a program with implicit region information — and the information is actually there as a result of elaboration. Changing the behavior of its type inference algorithm means changing how a program is interpreted as an abbreviation. The approach with region inference is, therefore, reduced to the approach with the ordinary region-based memory management. That's why the approach isn't satisfying for the current curiosity.</p>
<h4>Motivation</h4>
<p>This is where we come to reach our motivation; We want to realize memory management in a natural-deduction based programming language, without adding annotations to its type system. We want to find the vocabulary for memory management <strong>in</strong> our language (= the intuitionistic logic). We want to realize memory management in a Curry-Howard-y way.</p>
<p>From this viewpoint, this article is something that answers to the requirement above in a positive way. This article is something that shows how to realize such a resource management system, with an accompanying proof-of-concept implementation. A real programming language can still live in the ice-cold correspondence, after all.</p>
<p>This finally gives the background motivation to this article. It was a long run.</p>
<p>— But why after all do we use η-expansion rather than anything else? As we've already seen in the main part, we can leverage η-expansion to use the resource information of a type. Why thinking about η-expansion is related to thinking about resource management? We'll focus on this point in the next section.</p>
<h3>Computational significance of local completeness</h3>
<h4>Local soundness</h4>
<p>Let's go back to the proof-theoretic talk<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. We've already seen the concept of reduction. An operation that resolves a detour in a proof tree. Let's focus on this. Now, for example, suppose that we want to add the logical connective "AND" to our logical system. We'll write the "AND" of <span class="math inline">\( A \)</span> and <span class="math inline">\( B \)</span> as <span class="math inline">\( A \land B \)</span>. How the introduction rule and the elimination rule of "<span class="math inline">\( \land \)</span>" should be?</p>
<p>Well, defining the introduction rule and the elimination rule itself is not that difficult. The introduction rule should be something like this:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \Gamma \vdash e_1 : A \)}
  \AxiomC{\( \Gamma \vdash e_2 : B \)}
  \BinaryInfC{\( \Gamma \vdash (e_1, e_2) : A \land B \)}
\end{prooftree}
\]</span></p>
<p>That is, we need to know <span class="math inline">\( A \)</span> and <span class="math inline">\( B \)</span> to derive <span class="math inline">\( A \land B \)</span>. The elimination rules can also be easily added as follows, for example:</p>
<p><span class="math display">\[
\newcommand{\andlet}[3]{\mathsf{let}\, #1 := #2\, \mathsf{in}\, #3}
\begin{prooftree}
  \AxiomC{\( \Gamma \vdash e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{L}\, e : A \)}
\end{prooftree}
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \Gamma \vdash e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{R}\, e : B \)}
\end{prooftree}
\]</span></p>
<p>The reduction rules for this logical connective will be something like this:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e_1 : A \)}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e_2 : B \)}
  \BinaryInfC{\( \Gamma \vdash (e_1, e_2) : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{L}(e_1, e_2) : A \)}
\end{prooftree}
\hspace{1em}
\leadsto
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e_1 : A \)}
\end{prooftree}
\]</span></p>
<p>and this:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e_1 : A \)}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e_2 : B \)}
  \BinaryInfC{\( \Gamma \vdash (e_1, e_2) : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{R}(e_1, e_2) : A \)}
\end{prooftree}
\hspace{1em}
\leadsto
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e_2 : B \)}
\end{prooftree}
\]</span></p>
<p>No surprises until here.</p>
<p>Now, let's sell our souls to the devil and consider replacing the introduction rule of "<span class="math inline">\( \land \)</span>" by the following two rules:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \Gamma \vdash e : A \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{magic}_\mathsf{L}\, e : A \land B \)}
\end{prooftree}
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \Gamma \vdash e : B \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{magic}_\mathsf{R}\, e : A \land B \)}
\end{prooftree}
\]</span></p>
<p>Yes, broken, obviously. We don't even know how to interpret them. These magics are so broken that we can derive any proposition <span class="math inline">\( B \)</span> from any proposition <span class="math inline">\( A \)</span>:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \Gamma \vdash e : A \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{magic}_\mathsf{L}\, e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{R}(\mathsf{magic}_\mathsf{L}\, e) : B \)}
\end{prooftree}
\]</span></p>
<p>They can break the logical system behind them. We shouldn't accept such a pair of the introduction rules and the elimination rules.</p>
<p>The experiment above tells us that there must be certain relations between the introduction rules and the elimination rules of a logical connective, and that without it we would have a broken, insane, unsound logical connective. Then, in turn, what sort of relations does a logical connective need for it to be sane?</p>
<p>What if we just say "It'll break the system. I know. So what?" here? Let's try continuing the talk on our broken "<span class="math inline">\( \land \)</span>". As in the case of ordinary "<span class="math inline">\( \land \)</span>", we'll have to define its reduction rules for this insane logical connective. The detours for this connective are the following four, resulting from the two introduction rules and the two elimination rules:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : A \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{magic}_\mathsf{L}\, e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{L}(\mathsf{magic}_\mathsf{L}\, e) : A \)}
\end{prooftree}
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : A \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{magic}_\mathsf{L}\, e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{R}(\mathsf{magic}_\mathsf{L}\, e) : B \)}
\end{prooftree}
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : B \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{magic}_\mathsf{R}\, e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{L}(\mathsf{magic}_\mathsf{R}\, e) : A \)}
\end{prooftree}
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : B \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{magic}_\mathsf{R}\, e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{R}(\mathsf{magic}_\mathsf{R}\, e) : B \)}
\end{prooftree}
\]</span></p>
<p>The reduction rule for the first one should be easily defined as follows:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : A \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{magic}_\mathsf{L}\, e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{L}(\mathsf{magic}_\mathsf{L}\, e) : A \)}
\end{prooftree}
\hspace{1em}
\leadsto
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : A \)}
\end{prooftree}
\]</span></p>
<p>Similarly for the fourth rule:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : B \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{magic}_\mathsf{R}\, e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{R}(\mathsf{magic}_\mathsf{R}\, e) : B \)}
\end{prooftree}
\hspace{1em}
\leadsto
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : B \)}
\end{prooftree}
\]</span></p>
<p>The second and the third one are, however, problematic. Let's take the second one for example:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : A \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{magic}_\mathsf{L}\, e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{R}(\mathsf{magic}_\mathsf{L}\, e) : B \)}
\end{prooftree}
\hspace{1em}
\leadsto
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{?} : B \)}
\end{prooftree}
\]</span></p>
<p>We can't resolve the detour above. That's because, to resolve the detour, we need to prove the conclusion of the elimination rule (here it's <span class="math inline">\( B \)</span>) from the premise of the introduction rule (here it's <span class="math inline">\( A \)</span>), which is impossible. As a comparison, let's review the reduction of the ordinary "<span class="math inline">\( \land \)</span>":</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e_1 : A \)}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e_2 : B \)}
  \BinaryInfC{\( \Gamma \vdash (e_1, e_2) : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{L}(e_1, e_2) : A \)}
\end{prooftree}
\hspace{1em}
\leadsto
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e_1 : A \)}
\end{prooftree}
\]</span></p>
<p>We can see that the conclusion of the elimination rule (here it's <span class="math inline">\( A \)</span>) is shown using the premises of the introduction rules (here they're <span class="math inline">\( A \)</span> and <span class="math inline">\( B \)</span>, though only <span class="math inline">\( A \)</span> is used this time).</p>
<p>After all, the operation of resolving a detour is the operation of proving the conclusion of an elimination rule from the premises of the introduction rules<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. That is to say, we can define the reduction rules only when we can prove all the possible results of elimination from the premises of the introduction rule. That is to say, the elimination rules shouldn't be too strong with respect to the introduction rules — if an elimination rule is too strong, it can derive a proposition that strays from the range that is covered by the premises of an introduction rule.</p>
<p>As for the ordinary "<span class="math inline">\( \land \)</span>", all the possible propositions obtained by eliminating <span class="math inline">\( A \land B \)</span> — which is <span class="math inline">\( A \)</span> and <span class="math inline">\( B \)</span> — must be shown from the premises of the introduction rule — which is <span class="math inline">\( A \)</span> and <span class="math inline">\( B \)</span>. This property is indeed satisfied, and that's why we can define the reduction rule of "<span class="math inline">\( \land \)</span>".</p>
<p>On the other hand, as for the crazy "<span class="math inline">\( \land \)</span>", the proposition <span class="math inline">\( B \)</span> obtained by eliminating <span class="math inline">\( A \land B \)</span> can't be show from the premise of its introduction rule when the premise of the introduction rule is <span class="math inline">\( A \)</span>:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : A \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{magic}_\mathsf{L}\, e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \pi_\mathsf{R}(\mathsf{magic}_\mathsf{L}\, e) : B \)}
\end{prooftree}
\hspace{1em}
\leadsto
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{?} : B \)}
\end{prooftree}
\]</span></p>
<p>That's why we can't define the reduction rule for this connective.</p>
<p>A logical connective for which we can define the reduction rules is said to have local soundness. The lack of local soundness means the possibility of deriving propositions that shouldn't be derived. We can summarize here that our broken logical connective is broken in that it doesn't have local soundness.</p>
<h4>Local completeness</h4>
<p>By summarizing things in the way above, now we can consider a property that can be obtained by reversing local soundness. Or equivalently, a property that ensures that the elimination rule is not too weak. This property is called as local soundness.</p>
<p>Remember that we can characterize the property of local soundness as a possibility of rewriting proof trees — the possibility of reduction. Similarly, we can characterize the property of local completeness as a possibility of rewriting proof trees. We'll explain the characterization taking <span class="math inline">\( A \land B \)</span> as an example. Assume that we have a proof tree of <span class="math inline">\( e : A \land B \)</span>. In this situation, suppose that we can construct a proof tree of <span class="math inline">\( A \land B \)</span> that satisfies both of the followings:</p>
<ol>
<li>Every occurrence of a premise in the proof tree is of the form <span class="math inline">\( e : A \land B \)</span><br />
</li>
<li>Every occurrence of a premise <span class="math inline">\( e : A \land B \)</span> is immediately eliminated</li>
</ol>
<p>In this situation, we say that the "<span class="math inline">\( \land \)</span>" has local soundness. This mysterious definition needs an explanation, of course. Let's start from performing such a rewriting operation (expansion) to <span class="math inline">\( A \land B \)</span>:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : A \land B \)}
\end{prooftree}
\hspace{1em}
\leadsto
\hspace{1em}
\begin{prooftree}
  \AxiomC{\( \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{left}\, e : A \)}
  \AxiomC{\(  \vdots \)}
  \UnaryInfC{\( \Gamma \vdash e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{right}\, e : B \)}
  \BinaryInfC{\( \Gamma \vdash (\mathsf{left}\, e, \mathsf{right}\, e) : A \land B \)}
\end{prooftree}
\]</span></p>
<p>We can easily check that the expanded proof tree of <span class="math inline">\( A \land B\)</span> satisfies the properties above.</p>
<p>Now, suppose that the elimination rule is too weak. In this case, by the condition "every <span class="math inline">\( A \land B \)</span> is immediately eliminated", the information of <span class="math inline">\( A \land B \)</span> can only be used in some incomplete way. That is to say, we shouldn't be able to recover a proof of <span class="math inline">\( A \land B \)</span> in this case. For example, consider removing the following rule from the elimination rule of <span class="math inline">\( A \land B \)</span>:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \Gamma \vdash e : A \land B \)}
  \UnaryInfC{\( \Gamma \vdash \mathsf{right}\, e : B \)}
\end{prooftree}
\]</span></p>
<p>In this case, we can't construct the branch of <span class="math inline">\( \mathsf{right}\, e : B \)</span>, and we can't recover "<span class="math inline">\( A \land B \)</span>". Thus the "<span class="math inline">\( \land \)</span>" in this case doesn't have local completeness — its elimination rule is too weak.</p>
<p>Conversely, by the fact that we can recover <span class="math inline">\( A \land B \)</span>, we can see that the elimination rules of <span class="math inline">\( A \land B \)</span> is not too weak. That's why the possibility of such an expansion supports the local soundness of "<span class="math inline">\( \land \)</span>".</p>
<h4>Local soundness and time / local completeness and space</h4>
<p>Generally speaking, we don't much care about local completeness (and its corresponding expansion) when we think about the operational behavior of a program. It's almost ignored, and we often focus only on reduction. Well, that can be an overstatement, but there's that sort of vibe, I believe. Indeed, for example, when we want to think about the operational behavior of a programming language which is based on a certain form of lambda calculus, we only have to focus on its reduction, and we can simply ignore its local completeness.</p>
<p>At the same time, however, I find it a little mysterious. It's ultimately just our convenience that we exclusively utilize the reductional aspect of a logical system. The logical system won't give a damn to our these busy activities, so to speak. It might be, then, reasonable to some degree to think that the expanding operation can have as much significance as the reducing operation. They're the two sides of the same coin, after all<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. That's how I've come to think that local expansion could be utilized to realize some important aspect of computation, and that the aspect should be something that complements the reductional aspect.</p>
<p>Now, by the way, it won't be a sin to say that reduction is something that rules over the behavior of a program with respect to time. What can be, then, obtained by taking the other side of time? If we bring a computer scientist here and ask them about it, we'll see them saying that it's space. This pushes us even further: We might be able to utilize local completeness to realize memory management.</p>
<p>From this viewpoint, this article is something that does realize static memory management – control the behavior of a program with respect to space — via local completeness. In other words, this article gives support to the following (dubious) contrast:</p>
<ul>
<li>Local soundness gives a foundation of a program with respect to time<br />
</li>
<li>Local completeness gives a foundation of a program with respect to space</li>
</ul>
<p>Well, I know that this part is too rough and unsupported. It's unsupported enough to make me think that I should insert some self-ironical meme here, but unfortunately I can't since I'm not familiar with English memes. Anyway, having said that though, it's also a fact that the starting point of this article is around here, and that's why I chose to write down these thoughts, despite its sketchiness.</p>
<h3>A foundation for the resource management via copy/discard</h3>
<p>Let's move on to the next topic. Here we'll see an additional explanation of an aspect of the method in this article. That is, we'll see what kind of foundation can be found for the copy/discard approach.</p>
<p>Let's take the following ordinary reduction in lambda calculus as an example:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>   (λ x. (x, x)) @ &quot;hello&quot;</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>~&gt; (&quot;hello&quot;, &quot;hello&quot;)</span></code></pre></div>
<p>We name both of the terms in the reduction above as follows, just for explanation purpose:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>e1 := (λ x. (x, x)) @ &quot;hello&quot;</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>e2 := (&quot;hello&quot;, &quot;hello&quot;)</span></code></pre></div>
<p>The reduction above can be written as <code>e1 ~&gt; e2</code>, of course.</p>
<p>Now, consider comparing the behavior of the code <code>e1</code> and the code <code>e2</code> in a pure language with garbage collection. In this case, obviously, the behaviors of them are different in whether the computation that corresponds to the reduction above will occur or not. True, but that isn't the end of the story; We still have the following difference in space:</p>
<ul>
<li>When we write <code>e1</code> in our code, the string <code>"hello"</code> is, once created, shared in <code>(x, x)</code>. That is, when the program is in the state that corresponds to <code>("hello", "hello")</code>, the first and the second element of the tuple refer to the same memory region (the address of the string <code>"hello"</code>).<br />
</li>
<li>When we write <code>e2</code> in our code, the string <code>"hello"</code> is simply created twice.</li>
</ul>
<p>The string <code>"hello"</code> is created only once in the code <code>e1</code>, but twice in the code <code>e2</code>. That is to say, the behavior of a program with respect to space varies, depending on whether we write the former term or the latter term of the reduction <code>e1 ~&gt; e2</code>. That is to say again, the reduction doesn't preserve the result of computation with respect to space.</p>
<p>Then what will happen when we require the reduction rule to preserve the result of a program with respect to not only time but also space? In this case, the code <code>e1</code> must create the same number of copies of the string <code>"hello"</code> as that of <code>e2</code>. That is, 2 copies, and this "2" comes from, of course, the number of the <code>x</code> in the following code:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>λ x. (x, x)</span></code></pre></div>
<p>Thus the behavior of <code>e1</code> must be something as follows (adopting call-by-value):</p>
<ol>
<li>A piece of memory region for the string <code>"hello"</code> is allocated and initialized<br />
</li>
<li>The string is passed to the lambda abstraction<br />
</li>
<li>(Let n be the number of the times that the variable <code>x</code> is used in the lambda abstraction)<br />
</li>
<li>The lambda abstraction copies / discards the argument (= the string) to create n copies of it<br />
</li>
<li>Execute the remaining computation</li>
</ol>
<p>In this interpretation, the <code>e1</code> indeed creates two copies of <code>"hello"</code>. This gives a foundation for the copy/discard approach. That is, the approach is something that is automatically required when we request the reduction rule to preserve the result of computation with respect to not only time but also space.</p>
<h3>Can we expect acceptable performance from this approach?</h3>
<p>By the way, thinking things soberly, it is nothing but a crazy deed to copy the value of a variable every time when we use it. The word "wasteful" can't be enough for this. It's a violation of CPU rights. One might be lead to say that the approach presented in this article is something that can be meaningful only in the ivory tower, and that it is infeasible in our rough real world. The actual situation is, however, not that tragic. As is often the case, the ivory tower has a secret passage; Optimization.</p>
<p>We'll see three possible optimizations below. The most interesting aspect of this section would be, rather than the detail of each optimization, the fact that all of those optimizations are formulated by the words that specify how we should write a term in our language. The fact that we recognize vocabulary for resource management <strong>in</strong> the existing lambda calculus.</p>
<p>As expected, we won't see any benchmarks or that sort of thing here. Again, please take the content below with a grain of salt, and a bag of popcorn if you like.</p>
<h4>Borrowing-like operation</h4>
<p>The first one. Consider the following code:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>let str := &quot;hello&quot; in</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>let _ := print str in</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>let _ := print str in</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>print str</span></code></pre></div>
<p>The code above prints <code>"hello"</code> for the three times. We can see that the variable <code>str</code> is used for the three times. Thus the code above creates three copies of <code>"hello"</code>. That just sounds terrible.</p>
<p>We can, however, avoid this situation with a little thought. The point here is the type of <code>print</code>. We can set the type of <code>print</code> not <code>string -&gt; top</code>, but <code>string -&gt; string * top</code>. That is, we set <code>print</code> as a primitive function with the following behavior:</p>
<ol>
<li>Receives a string <code>s</code> as the argument of it<br />
</li>
<li>Prints <code>s</code><br />
</li>
<li>Returns the pair of <code>s</code> and <code>unit</code><br />
</li>
</ol>
<p>With this <code>print</code>, the code above can be rewritten as follows:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>let str := &quot;hello&quot; in</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>let (str1, _) := print str in</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>let (str2, _) := print str1 in</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>print str2</span></code></pre></div>
<p>Or, renaming the variables,</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>let str := &quot;hello&quot; in</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>let (str, _) := print str in</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>let (str, _) := print str in</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>print str</span></code></pre></div>
<p>With this rewriting, we can avoid the copying operations for <code>"hello"</code> in the original code.</p>
<p>By the way, I noticed that the above pattern of "attaching an argument to its result as it is" occurs frequently. That's why I added a dedicated syntax for it in the language that I implemented this time. That is, if we write a term something like below:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>let _ := print &amp;str in (...)</span></code></pre></div>
<p>This term is, then, translated into the term:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>let (str, _) := print str in (...)</span></code></pre></div>
<p>in the parsing stage (Although the actual looking is a little different since the actual syntax is S-expression based, what happens is essentially as above). It looks like borrowing, though I think I should refrain from using this word here so as not to cause the name collision to the proper ones in C++ or Rust.</p>
<p>Anyway, using this syntax, we can write, for example, a function that prints the received string twice as follows:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>let print_twice :=</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>  λ str.</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    let _ := print &amp;str in</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    let _ := print &amp;str in</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    (str, top.unit)</span></code></pre></div>
<h4>State and shadowing</h4>
<p>The second one. Suppose that we want to realize a computation with states. In what way should we implement it? Of course, we can use the state monad, leveraging the power of the lambda calculus. This approach has, however, a problem. Let's see how the <code>get</code> of the state monad is implemented:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>let get :=</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>  λ s. (s, s)</span></code></pre></div>
<p>Here the <code>s</code> is a variable that receives the state. If the state monad has, for example, <code>string</code> as its state, then the <code>s</code> receives a string. Now the problem should be clear: Since the variable <code>s</code> is used in a non-linear way (twice), the string is copied every time when we call the function <code>get</code>. The state is copied as a whole every time when we use the state. A tragedy. That makes us seek an alternative approach.</p>
<p>And luckily there indeed is such an alternative approach: We just have to apply the shadowing-based optimization that we've seen above to a top-level variable. I think this can be best explained by an example:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>-- define a variable to keep state information</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>let str-state :=</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>  &quot;hello&quot; in</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>-- define a function that receives the state `s`, compute using that state, and returns the pair `({updated state}, unit)`</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>let proc :=</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>  λ s.</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>    let _ := print &amp;s in</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>    let s := concat s &quot;!&quot; -- append &quot;!&quot; to the current state (string)</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>    let _ := print &amp;s in</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>    (s, top.unit) in</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>-- use the borrowing-like operation at top level</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>let _ := proc &amp;str-state in</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>-- the above is equivalent to `let (str-state, _) := proc str-state`, and thus str-state is &quot;hello!&quot; here</span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>-- do the same thing again</span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a>let _ := proc &amp;str-state in</span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a>-- str-state is &quot;hello!!&quot; here</span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a>(...)</span></code></pre></div>
<p>Note that the variable for a state (<code>str-state</code> in this case) is used linearly, that we can read the value of the state, and that we can "write" a new value to the state. This allows us to realize a state-related computation without causing wasteful copy operations of the state. We can easily add a syntax sugar for this kind of operation, of course. This resolves the problem of computation with states.</p>
<h4>Cancelling malloc/free</h4>
<p>The third one. Remember that our language determines memory allocation/deallocation in a static way. This means, as a matter of course, that we should know, at the time when the compilation is finished, where to insert <code>mallloc</code> s and <code>free</code> s, and the size of the memory region that a malloc/free should handle. Thus the resulting code will be conceptually something like this:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>a := malloc(SIZE);</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>(...)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>free(a);</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>b := malloc(SIZE);</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>(...)</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>free(b);</span></code></pre></div>
<p>Here, since the <code>a</code> and <code>b</code> are of the same size, we can reuse the region of <code>a</code> for <code>b</code>. Thus, the code above should be able to be rewritten as follows:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>a := malloc(SIZE);</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>(...)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>b := a;</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>(...)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>free(b);</span></code></pre></div>
<p>And this is indeed possible. This optimization is implemented in the actual language.</p>
<h4>Summary of performance</h4>
<p>So, to what extent these optimizations work well in real life? Can we expect acceptable performance from this approach? The most honest answer from me is, as expected, the "I don't know." Personally, I'm thinking that the range that can be covered by the linear lambda calculus is, thanks to the method that we've seen above, actually larger than we would expect at first, but this is nothing but a sheer imagination that isn't supported by anything. I, of course, want to develop the current implementation into a more full-fledged one so that I can test its performance using some profiling tools, but my lack of this and that resources don't allow me to do that.</p>
<p>Incidentally, it might also be of interest to investigate what will happen when we take multi-threaded behavior into account. I reckon that we can't send/receive states between threads as long as we realize the "write" operation on states by means of shadowing (since the result of the "write" operation in a thread can't be reflected in the other threads), but I don't know to what extent this difference is significant.</p>
<p>I also ran into <a href="https://arxiv.org/abs/1802.00961">a work that investigates multi-threaded behavior via the Curry-Howard correspondence</a>. The research utilizes a generalization of the Gödel's axiom (Here, the axiom is like <span class="math inline">\( (A \to B) \lor (B \to A) \)</span>). I find it intriguing. At the same time, however, I find the reduction rule in the research rather complex — at least for me. Although it might be possible that if I fully understand the work then the reduction rules can be derived immediately in some simple, principled way, currently I don't think that I can incorporate it into the language as it is. Or should I add mysterious constants and use them as black-boxes that realize multi-threaded behavior? I'm not sure.</p>
<h3>Miscellaneous</h3>
<p>Some additional notes, basically on things that I left undone.</p>
<h4>On safety</h4>
<p>As I wrote at the beginning of this article, I don't have any proof that guarantees the safety or the correctness of the approach in this article. This is simply a result of my limited resource.</p>
<p>Regarding safety, I think I should mention the inconsistency of the logic here. Since the logical system of the language admits <code>Type : Type</code> (Or, since it has a fixed point operator), the logical system is obviously inconsistent. That is, for any type <code>A</code>, we can construct a proof term <code>e</code> such that <code>e : A</code>. Also, remember that the resource management system in this article is based on type. This might pose a question: Can't we destroy the proof-theoretic resource management system by exploiting the inconsistency of the logic?</p>
<p>Trying it out in actual code, however, we can see that the execution of such <code>e</code> just results in an infinite loop. Admitting <code>Type : Type</code> doesn't mean admitting, for example, <code>"hello" : int</code>. The <code>e</code> in a proof <code>e : A</code> follows a certain pattern even when we construct the <code>e</code> using <code>Type : Type</code> or <code>fix</code>. Such a logic is consistently inconsistent, so to speak, though I don't have any proof for this, as usual.</p>
<h4>Where's a denotational semantics?</h4>
<p>As anyone with certain knowledge can see within 0.2 seconds, this article is heavily biased towards the syntax side, or the proof-theoretic side. Equivalently, this article doesn't contain any information on a denotational semantics of the system. I personally think that translating a type into a term is interesting in its own way, and am interested in its behavior from the viewpoint of categorical semantics. I'm, however, currently not very good at categorical semantics of dependent type theory.</p>
<h4>On the lack of theoretical backgrounds</h4>
<p>Related to the above-mentioned point, as you may have already noticed, the theoretical backgrounds of this article is far from complete. Being a little technical, the actual compiler behaves basically as follows:</p>
<ol>
<li>begins from receiving a program written in the Calculus of Constructions (an intuitionistic logic on steroids)<br />
</li>
<li>translates the program into the one in a dependent variant of the Call-By-Push-Value<br />
</li>
<li>applies closure conversion so that all the lambda abstractions in the program are closed<br />
</li>
<li>linearizes all the resources via our computational interpretation of type<br />
</li>
<li>generates virtual machine code as usual</li>
</ol>
<p>Frighteningly enough, I didn't give any proofs that guarantee the correctness of the translations included in the above. I didn't even check that the dependent variant of CBPV isn't broken. Generally speaking, as we all know, a seemingly well-designed idea in our mind is doomed to be found broken when we try to re-articulate it on a piece of paper; Even though we do have a working implementation, it is still possible that we find an inherent vice in the approach<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>.</p>
<h4>On the name</h4>
<p>Let me digress a little before concluding the article. I went with naming the language "Neut" this time. Here, I'd like to emphasize that I don't have any thought of, like, "This language is neutral in an important aspect!" or whatever. Indeed, the language is obviously opinionated in that, for example, it adopts the viewpoint of "I ♡ natural deduction". Rather, the name came from its implementation.</p>
<p>As mentioned above, the compiler translates an intuitionistic lambda calculus (the Calculus of Constructions) into a dependent variant of a calculus called Call-By-Push-Value (CBPV). The main difference between CBPV and an ordinary calculus would be the point that CBPV has two kinds of type. More specifically, CBPV has "type for value" and "type for computation"; The former are normally said to be positive, and the latter to be negative. That is, the language has polarity. The introduction rule of, for example, "<span class="math inline">\( \to \)</span>" in CBPV is as follows:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \Gamma, x : P \vdash e : N \)}
  \UnaryInfC{\( \Gamma \vdash \lambda x.e : P \to N \)}
\end{prooftree}
\]</span></p>
<p>Here, the <span class="math inline">\( P \)</span> stands for a positive type, and the <span class="math inline">\( N \)</span> for a negative type. We can easily see that the argument of a lambda abstraction must be positive (<span class="math inline">\( x : P \)</span>), and that the body of the abstraction must be negative (<span class="math inline">\( e : N \)</span>). Also, the <span class="math inline">\( P \to N \)</span> itself is set to be a negative type, and thus it can't be, for example, passed to another lambda abstraction as an argument without some modification.</p>
<p>If it's allowed to say that linear logic is something that enables us to investigate logic further with respect to resources, it might also be allowed to say that CBPV is something that enables us to investigate logic further with respect to reduction (too rough?). More information on CBPV can be found <a href="https://www.cs.cmu.edu/~fp/courses/15816-f16/lectures/21-cbpv.pdf">here</a>; The lecture note should be readable with basic knowledge of natural deduction. <a href="https://www.cs.bham.ac.uk/~pbl/papers/thesisqmwphd.pdf">The dissertation by Levy</a>, who created/discovered the CBPV, is even more detailed, though it can be overwhelming.</p>
<p>Anyway, the point here is that the compiler polarizes an input by a user. From this perspective, a program doesn't have polarity at first, and it is neutral in that sense. What a user writes is a neutral proof term. So, just like we write text in a text file and set its extension to be ".txt", I chose to set the extension of a file that write a neutral term ".neut", and also went with using it as the name of the language. That's the whole story.</p>
<h2>Afterword</h2>
<p>GG.</p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>When trying to recover the expressivity of intuitionistic logic starting from linear logic, the first approach that one comes up with would be to use the famous exponential modality <code>!</code>. This approach, however, doesn't seem to work well. Consider adding the modality to the system, and admitting the weakening/contraction operation for the propositions with the modality (that is, the propositions of the form <code>!A</code>). In this situation, the modified system essentially contains the intuitionistic logic as a fragment — a fragment in which all the propositions are of the form <code>!A</code>. This makes us go back to the starting point: "How can we manage memory in a system based on the intuitionistic logic?"<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>A question can be posed here: What will happen when a type contains free variables? The short answer is that this sort of situation is handled by the generalized concept of a free variable. That is, the concept is generalized so that free variables in a type of a term are also regarded as free variables of the term. For example, consider the following term:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>λ (A : Type). λ (x : A -&gt; int). (x, 100)</span></code></pre></div>
<p>Normally, the free variable of <code>(x, 100)</code> is considered to be simply <code>x</code>. With our generalization, however, since the type of <code>x</code> (i.e. <code>A -&gt; int</code>) contains <code>A</code> as its free variable, this <code>A</code> is also counted as a free variable. Also, since the type of <code>A</code> (i.e. <code>Type</code>) doesn't contain any free variables, this "tracing" operation ends here, with the result that the free variables of <code>(x, 100)</code> is <code>A, x</code>. In this way, we construct a "closed chain" of all the free variables of a term. We then put it at the first element of a generalized closure so that we can copy the closure later.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>This extension of closure conversion is essentially just an existential quantification. Our familiar existential quantification starts from a statement with free variable(s) like <code>x + 1 = 0</code>, and then create a proposition like <code>exists (x : int). x + 1 = 0</code>, anonymizing the variable <code>x</code>. What the generalized closure conversion does is essentially the same; The type of the closure</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>({the set of free variables},</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a> λ (the original arguments, env).</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>   let (the free variables) := env in</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>   {the original code})</span></code></pre></div>
<p>is like:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>(A1 * ... * An) * {the type of the closed function}</span></code></pre></div>
<p>The conversion here is something that anonymizes the first element with the existential quantification, creating a term of the following type:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>Sigma (A : Type). A * {the type of the closed function}</span></code></pre></div>
<p>The point here is that the <code>A1 * ... * An</code> is lowered to the term from the type.</p>
<p>Incidentally, this 3-element representation of a closure is not new. Indeed, I was told in <a href="https://github.com/u2zv1wx/neut/issues/1">a GitHub issue</a> that there exists <a href="https://www.ccs.neu.edu/home/amal/papers/closconvcc.pdf">a work</a> that does a similar (the same?) thing. Also, with a quick search, I found <a href="https://sv.c.titech.ac.jp/minamide/papers/popl96.pdf">a work in 1996 that does a similar thing</a>. Thus I emphasize here that I don't claim any originality on this generalization of closure conversion.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>The actual compiler also uses GC so that it can make its output as efficient as possible. I think this can be understood as an optimization, though.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>The content here is mainly based on <a href="https://www.cs.cmu.edu/~fp/papers/mscs00.pdf">this paper</a>, except for the last sketchy part.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>More precisely, the operation of proving the conclusion of an elimination rule using the premises of the introduction rule and the additional premises of the elimination rule. Let's take the elimination rule of "<span class="math inline">\( \to \)</span>" for example:</p>
<p><span class="math display">\[
\begin{prooftree}
  \AxiomC{\( \Gamma \vdash A \to B \)}
  \AxiomC{\( \Gamma \vdash A \)}
  \RightLabel{\( (\to_{\mathsf{e}}) \)}
  \BinaryInfC{\( \Gamma \vdash B \)}
\end{prooftree}
\]</span></p>
<p>The elimination rule has <span class="math inline">\( A \to B \)</span> and <span class="math inline">\( A \)</span> as its premises. "The additional premises of the elimination rule" is the <span class="math inline">\( A \)</span> here. The example of "<span class="math inline">\( \land \)</span>" is a case in which we have 0 additional premises.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>I found an answer on Stack Exchange saying that <a href="https://math.stackexchange.com/questions/1633210/is-there-a-connection-between-local-soundness-and-completeness-in-proof-theory">they correspond to the triangle identities of an adjunction</a>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>What will happen when a type contains side effects? There can be at least two responses to this situation:</p>
<ol>
<li>To reject such a term at the type checking stage by making the language pure<br />
</li>
<li>To set its behavior undefined, allowing such a term to pass the type checking stage</li>
</ol>
<p>The former approach would set, for example, the type of <code>print</code> to be <code>string -&gt; io (string * top)</code> (the <code>io</code> is our beloved IO monad). By this modification, the type of a "type" that uses <code>print</code> in its definition changes from <code>Type</code> to <code>IO Type</code>. Since the <code>A</code> in <code>A : IO Type</code> can't be used as a type, we can't write, for example, <code>A -&gt; int</code>. We're safe now.</p>
<p>Conversely, the latter approach just says: "It's your fault. Don't write crazy stuff in a type." The actual implementation adopts this approach, by the way.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

        </main>
        <footer>
            <p>
                <span class="metadata">
                    <a href="https://twitter.com/ricotie">twitter</a>
                    /
                    <a href="https://github.com/vekatze">github</a>
                    /
                    <a href="https://github.com/vekatze">email</a>
                </span>
            </p>
        </footer>
        
    </body>
</html>
